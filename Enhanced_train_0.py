import os
import argparse
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from torchvision.datasets import ImageFolder
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import sklearn.metrics as metrics
import seaborn as sns
import sys
# æ·»åŠ é¡¹ç›®è·¯å¾„åˆ°Pythonæœç´¢è·¯å¾„
PROJECT_PATH = '/kaggle/input/efficient-gate/EfficientNet-PyTorch-MLOps'
sys.path.insert(0, PROJECT_PATH)

# å¯¼å…¥é¡¹ç›®ä¸­çš„modelæ¨¡å—
from efficientnet_pytorch.model import EfficientNet
from tqdm import tqdm


class MultiHeadAttentionLayer(nn.Module):
    def __init__(self, in_channels, num_heads):
        super(MultiHeadAttentionLayer, self).__init__()
        assert in_channels % num_heads == 0, "in_channels must be divisible by num_heads"
        self.head_dim = in_channels // num_heads
        self.num_heads = num_heads

        self.conv1 = nn.Conv2d(in_channels, in_channels // 8, kernel_size=1)
        self.conv2 = nn.Conv2d(in_channels // 8, in_channels, kernel_size=1)

    def forward(self, x):
        B, C, H, W = x.size()
        attention_weights = torch.sigmoid(self.conv2(torch.relu(self.conv1(x))))
        attention_weights = attention_weights.view(B, self.num_heads, self.head_dim, H, W)
        attention_weights = attention_weights.view(B, C, H, W)
        return x * attention_weights


class EfficientNetWithAttention(nn.Module):
    def __init__(self, model_name='efficientnet-b0', num_classes=1000, num_heads=8):
        super(EfficientNetWithAttention, self).__init__()
        self.efficientnet = EfficientNet.from_pretrained(model_name, num_classes=num_classes)
        self.attention = MultiHeadAttentionLayer(in_channels=1280, num_heads=num_heads)            # ç½‘ç»œçš„è¾“å‡ºç»´åº¦

    def forward(self, x):
        x = self.efficientnet.extract_features(x)
        x = self.attention(x)
        x = self.efficientnet._avg_pooling(x)
        x = x.flatten(start_dim=1)
        x = self.efficientnet._dropout(x)
        x = self.efficientnet._fc(x)
        return x

    def extract_features(self, x):
        return self.efficientnet.extract_features(x)


# è¯„ä¼°æ¨¡å‹å‡½æ•°
# æ¨¡å‹è®­ç»ƒ
def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, device, epochs, num_classes):
    train_losses, val_losses = [], []
    train_accuracies, val_accuracies = [], []
    cm = np.zeros((num_classes, num_classes))  # åˆå§‹åŒ–æ··æ·†çŸ©é˜µ

    for epoch in range(epochs):
        model.train()
        running_loss = 0.0

        with tqdm(total=len(train_loader), desc=f'Epoch {epoch + 1}/{epochs}', unit='batch') as pbar:
            for images, labels in train_loader:
                images, labels = images.to(device), labels.to(device)
                optimizer.zero_grad()
                outputs = model(images)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()
                running_loss += loss.item()
                pbar.set_postfix(loss=running_loss / (pbar.n + 1))
                pbar.update(1)

        train_loss = running_loss / len(train_loader)
        train_losses.append(train_loss)

        # è¯„ä¼°è®­ç»ƒé›†å’ŒéªŒè¯é›†
        val_loss, val_accuracy, val_preds, val_labels = evaluate_model(model, val_loader, criterion, device)
        train_accuracy = evaluate_model(model, train_loader, criterion, device)[1]  # è·å–è®­ç»ƒå‡†ç¡®ç‡
        val_losses.append(val_loss)
        train_accuracies.append(train_accuracy)
        val_accuracies.append(val_accuracy)

        # # æ›´æ–°æ··æ·†çŸ©é˜µ
        # _, preds = torch.max(outputs, 1)
        # for true, pred in zip(labels.cpu().numpy(), preds.cpu().numpy()):
        #     cm[true, pred] += 1
        # æ›´æ–°æ··æ·†çŸ©é˜µï¼ˆä»…åœ¨éªŒè¯é›†ä¸Šï¼‰
        for true, pred in zip(val_labels, val_preds):
            cm[true, pred] += 1

        print(f'Epoch [{epoch + 1}/{epochs}], '
              f'Training Loss: {train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}, '
              f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')

        scheduler.step()

    # ä¿å­˜æŒ‡æ ‡å’Œæ··æ·†çŸ©é˜µ
    save_metrics(train_losses, val_losses, train_accuracies, val_accuracies, cm)

    return train_losses, val_losses, train_accuracies, val_accuracies


def evaluate_model(model, loader, criterion, device):
    model.eval()
    total_loss = 0.0
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for images, labels in loader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            loss = criterion(outputs, labels)
            total_loss += loss.item()
            _, preds = torch.max(outputs, 1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    avg_loss = total_loss / len(loader)
    accuracy = np.mean(np.array(all_preds) == np.array(all_labels))
    return avg_loss, accuracy, all_preds, all_labels  # è¿”å›æŸå¤±ã€å‡†ç¡®ç‡ã€é¢„æµ‹å’ŒçœŸå®æ ‡ç­¾


# æ•°æ®ä¿å­˜
def save_metrics(train_losses, val_losses, train_accuracies, val_accuracies, cm, filename_prefix='metrics'):
    # ä¿å­˜æŸå¤±å’Œå‡†ç¡®ç‡åˆ° CSV æ–‡ä»¶
    output_dir = '/kaggle/working/'
    metrics_df = pd.DataFrame({
        'Epoch': range(1, len(train_losses) + 1),
        'Train Loss': train_losses,
        'Val Loss': val_losses,
        'Train Accuracy': train_accuracies,
        'Val Accuracy': val_accuracies
    })
    metrics_df.to_csv(f'{output_dir}{filename_prefix}_metrics.csv', index=False)
    print(f'Metrics saved to {output_dir}{filename_prefix}_metrics.csv')

    # ä¿å­˜æ··æ·†çŸ©é˜µåˆ° TXT æ–‡ä»¶
    np.savetxt(f'{output_dir}{filename_prefix}_confusion_matrix.txt', cm, fmt='%d')
    print(f'Confusion matrix saved to {output_dir}{filename_prefix}_confusion_matrix.txt')


# ç»˜åˆ¶æŸå¤±å’Œå‡†ç¡®ç‡çš„å‡½æ•°
def plot_metrics(train_losses, val_losses, train_accuracies, val_accuracies, epochs):
    # ç»˜åˆ¶æŸå¤±æ›²çº¿
    plt.figure(figsize=(10, 6))
    plt.plot(range(1, epochs + 1), train_losses, label='Training Loss', color='blue')
    plt.plot(range(1, epochs + 1), val_losses, label='Validation Loss', color='red')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.ylim(0, 1)
    plt.xticks(range(0, epochs + 1, 10))  # xè½´åˆ»åº¦ä¸º0, 10, 20, ...
    plt.yticks(np.arange(0, 1.1, 0.2))  # yè½´åˆ»åº¦ä¸º0.0, 0.2, 0.4, ..., 1.0
    plt.legend()
    plt.grid(False)
    plt.show()

    # ç»˜åˆ¶å‡†ç¡®ç‡æ›²çº¿
    plt.figure(figsize=(10, 6))
    plt.plot(range(1, epochs + 1), train_accuracies, label='Training Accuracy', color='blue')
    plt.plot(range(1, epochs + 1), val_accuracies, label='Validation Accuracy', color='red')
    plt.title('Training and Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.ylim(0, 1)
    plt.xticks(range(0, epochs + 1, 10))  # xè½´åˆ»åº¦ä¸º0, 10, 20, ...
    plt.yticks(np.arange(0, 1.1, 0.2))  # yè½´åˆ»åº¦ä¸º0.0, 0.2, 0.4, ..., 1.0
    plt.legend()
    plt.grid(False)
    plt.show()


# ç»˜åˆ¶æ··æ·†çŸ©é˜µçš„å‡½æ•°
def plot_confusion_matrix(cm, class_names):
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=class_names, yticklabels=class_names)
    plt.xlabel('Predicted Label')
    plt.ylabel('True Label')
    plt.title('Confusion Matrix')
    plt.show()


# æå–ç‰¹å¾å‘é‡çš„å‡½æ•°
def extract_features(model, loader, device):
    model.eval()  # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
    all_features = []
    all_labels = []

    with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è®¡ç®—
        for images, labels in loader:
            images = images.to(device)

            # æå–ä¸­é—´å±‚è¾“å‡ºï¼ˆç‰¹å¾å‘é‡ï¼‰
            features = model.extract_features(images)
            features = features.view(features.size(0), -1)  # å°†ç‰¹å¾å‘é‡å±•å¹³ä¸ºäºŒç»´

            all_features.extend(features.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    return np.array(all_features), np.array(all_labels)


# ä¿å­˜ç‰¹å¾å‘é‡åˆ°æ–‡ä»¶
def save_features(features, labels, filename='val_features.npy'):
    output_dir = '/kaggle/working/'
    np.save(f'{output_dir}{filename}', {'features': features, 'labels': labels})
    print(f"Features saved to {output_dir}{filename}")


def main(args, num_classes):
    # è®¾ç½®Kaggleç¯å¢ƒæ ‡è¯†
    os.environ['IN_KAGGLE'] = '1'
    print(f"Using device: {args.device}")
    print(f"Running in Kaggle environment: {os.environ.get('IN_KAGGLE', '0')}")
    print(f"Project path: {PROJECT_PATH}")
    print("ğŸ¯ è®­ç»ƒæ¨¡å¼: ä»å¤´è®­ç»ƒ EfficientNet-B0 (å•é€šé“è¾“å…¥ï¼Œæ— é¢„è®­ç»ƒ)")

    # å•é€šé“å›¾åƒé¢„å¤„ç†ï¼ˆé€‚ç”¨äºRPå›¾åƒç­‰ç°åº¦å›¾ï¼‰
    transform = transforms.Compose([
        transforms.Resize((args.image_size, args.image_size)),
        transforms.Grayscale(num_output_channels=1),  # è½¬æ¢ä¸ºå•é€šé“
        transforms.ToTensor(),
        # å•é€šé“æ ‡å‡†åŒ–ï¼ˆä½¿ç”¨ImageNet RGBçš„å¹³å‡å€¼çš„å¹³å‡ä½œä¸ºç°åº¦å€¼ï¼‰
        transforms.Normalize(mean=[0.449], std=[0.227])
    ])

    train_dataset = ImageFolder(os.path.join(args.data, 'train'), transform=transform)
    val_dataset = ImageFolder(os.path.join(args.data, 'val'), transform=transform)

    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=args.workers)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)

    # ä»å¤´è®­ç»ƒEfficientNet-B0ï¼ˆå•é€šé“è¾“å…¥ï¼Œæ— é¢„è®­ç»ƒæƒé‡ï¼‰
    print("ğŸ”„ åˆ›å»ºä»å¤´è®­ç»ƒçš„EfficientNet-B0æ¨¡å‹ï¼ˆå•é€šé“è¾“å…¥ï¼‰...")
    model = EfficientNetWithAttention(model_name='efficientnet-b0', num_classes=num_classes, num_heads=args.num_heads)
    
    # ä¿®æ”¹ç¬¬ä¸€ä¸ªå·ç§¯å±‚ä»¥é€‚åº”å•é€šé“è¾“å…¥
    # åŸæ¥çš„conv_stemæœŸå¾…3é€šé“è¾“å…¥ï¼Œç°åœ¨æ”¹ä¸º1é€šé“
    original_conv_stem = model.efficientnet._conv_stem
    new_conv_stem = nn.Conv2d(
        1,  # è¾“å…¥é€šé“æ”¹ä¸º1
        original_conv_stem.out_channels,
        kernel_size=original_conv_stem.kernel_size,
        stride=original_conv_stem.stride,
        padding=original_conv_stem.padding,
        bias=original_conv_stem.bias is not None
    )
    # åˆå§‹åŒ–æ–°å·ç§¯å±‚æƒé‡ï¼ˆå°†åŸæ¥3é€šé“çš„æƒé‡å¹³å‡åˆ†é…åˆ°1é€šé“ï¼‰
    with torch.no_grad():
        new_conv_stem.weight[:, 0, :, :] = original_conv_stem.weight.mean(dim=1)
        if original_conv_stem.bias is not None:
            new_conv_stem.bias = original_conv_stem.bias
    
    model.efficientnet._conv_stem = new_conv_stem
    print("âœ… å•é€šé“è¾“å…¥å±‚ä¿®æ”¹å®Œæˆ")
    
    model = model.to(args.device)

    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=args.lr)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)

    # å°†num_classesä¼ é€’ç»™train_modelå‡½æ•°
    train_losses, val_losses, train_accuracies, val_accuracies = train_model(
        model, train_loader, val_loader, criterion, optimizer, scheduler, args.device, args.epochs, num_classes
    )

    # ç»˜åˆ¶è®­ç»ƒå’ŒéªŒè¯æŸå¤±åŠå‡†ç¡®ç‡
    plot_metrics(train_losses, val_losses, train_accuracies, val_accuracies, args.epochs)

    # æå–éªŒè¯é›†ç‰¹å¾
    val_features, val_labels = extract_features(model, val_loader, args.device)
    save_features(val_features, val_labels, filename='val_features.npy')

    # è¯„ä¼°æ¨¡å‹å¹¶ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    val_loss, val_accuracy, val_preds, val_labels = evaluate_model(model, val_loader, criterion, args.device)
    cm = metrics.confusion_matrix(val_labels, val_preds)

    # ä¿å­˜æŒ‡æ ‡å’Œæ··æ·†çŸ©é˜µ
    save_metrics(train_losses, val_losses, train_accuracies, val_accuracies, cm)

    # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    plot_confusion_matrix(cm, class_names=[str(i) for i in range(len(val_dataset.classes))])


if __name__ == '__main__':
    # Jupyter/Colabå…¼å®¹æ€§å¤„ç†
    import sys
    
    # æ£€æŸ¥æ˜¯å¦åœ¨Jupyterç¯å¢ƒä¸­
    if 'ipykernel' in sys.modules or 'colab' in sys.modules:
        print("æ£€æµ‹åˆ°Jupyter/Colabç¯å¢ƒï¼Œä½¿ç”¨é»˜è®¤å‚æ•°...")
        
        # åˆ›å»ºé»˜è®¤å‚æ•°å¯¹è±¡
        class Args:
            def __init__(self):
                self.data = '/kaggle/input/your-dataset/data'
                self.epochs = 50  # å‡å°‘é»˜è®¤è½®æ¬¡
                self.batch_size = 16
                self.lr = 0.0005
                self.workers = 2  # å‡å°‘workeræ•°é‡
                self.image_size = 224
                self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
                self.num_heads = 8
        
        args = Args()
        num_classes = 6
        main(args, num_classes)
    else:
        # å‘½ä»¤è¡Œæ¨¡å¼
        parser = argparse.ArgumentParser(description='EfficientNet Classification')
        parser.add_argument('--data', metavar='DIR',
                            default='/kaggle/input/your-dataset/data',
                            help='path to dataset')
        parser.add_argument('--epochs', default=100, type=int, help='number of total epochs to run')
        parser.add_argument('--batch-size', default=16, type=int, help='batch size')
        parser.add_argument('--lr', default=0.0005, type=float, help='initial learning rate')
        parser.add_argument('--workers', default=4, type=int, help='number of data loading workers')
        parser.add_argument('--image_size', default=224, type=int, help='image size')
        parser.add_argument('--device', default='cuda' if torch.cuda.is_available() else 'cpu',
                            help='device to use for training')
        parser.add_argument('--num_heads', default=8, type=int, help='number of attention heads')

        num_classes = 6  # è¿™é‡Œè®¾ç½®åˆ†ç±»çš„ç±»åˆ«

        args = parser.parse_args()
        main(args, num_classes)

